{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bayt logo](https://images.g2crowd.com/uploads/product/image/social_landscape/social_landscape_b9320393d0092c1c2f9f5e1d78197b14/bayt-com.png)\n",
    "\n",
    "# Bayt website links scrapping\n",
    "**Note: All the links were collected on 20-12-2022**\n",
    "\n",
    "In this notebook we will scrape all the job links in bayt website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xq7wx8vCbrI0"
   },
   "outputs": [],
   "source": [
    "# import all the needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import numpy as np\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the jobs pages from the site.\n",
    "#### The number 304 is number of pages in the site you can go to this [link](https://www.bayt.com/en/egypt/jobs/?page=1) and get the last page number and put it instead of 304 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63ssOCgtcoSY"
   },
   "outputs": [],
   "source": [
    "urls = [f'https://www.bayt.com/en/egypt/jobs/?page={i}' for i in range(1, 304)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWzWd9PGdjkQ"
   },
   "outputs": [],
   "source": [
    "def get_links(urls, thread_number):\n",
    "    # create or open new csv file to save all the links\n",
    "    csv_file = open(f'bayt_job_links_20-12-2022_{thread_number}.csv', 'a+',encoding='utf-16')\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=['title', 'link'])\n",
    "    writer.writeheader()\n",
    "    for url_index, url in enumerate(urls):\n",
    "        # make a request to the page to get the data to be scraped\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # convert the response to soup object to be able to extract information form it\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        jobs = soup.find_all('li', class_='has-pointer-d')\n",
    "        print(f'Thread #{thread_number}: start url: {url_index}')\n",
    "        \n",
    "        # start the link and title extraction form the page\n",
    "        for job_index, job in enumerate(jobs):\n",
    "            print(f'Thread #{thread_number}: Job: {job_index}')\n",
    "            title = job.find('h2', class_='jb-title').text.strip()\n",
    "            link = job.find('h2', class_='jb-title').a.get('href')\n",
    "            \n",
    "            # add the link to the csv file\n",
    "            writer.writerow({\n",
    "              'link': f'https://www.bayt.com{link}',\n",
    "              'title': title\n",
    "            })\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see there are alot of pages that I need to scrape and it could even get bigger so for ease of use and time I have used mutithreads to split the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Onh_J75IeJiq",
    "outputId": "33487dc1-7a40-459d-967e-56252c0e0d91"
   },
   "outputs": [],
   "source": [
    "splitied_urls = np.array_split(urls, 8) # I have used array_split to split my data to n (8) numbers of smaller lists you can change the 8 number to be number of threads you want to use\n",
    "threads = []\n",
    "# start the thread for each list in my case 8 lists so 8 threads\n",
    "for index, mini_urls in enumerate(splitied_urls):\n",
    "    thread = threading.Thread(target=get_links, kwargs={'urls': mini_urls, 'thread_number': str(index + 1)})\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "# after ending the application re join the threads again (like closing the threads)\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
