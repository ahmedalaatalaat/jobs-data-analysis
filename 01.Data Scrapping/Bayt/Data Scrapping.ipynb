{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bayt logo](https://images.g2crowd.com/uploads/product/image/social_landscape/social_landscape_b9320393d0092c1c2f9f5e1d78197b14/bayt-com.png)\n",
    "\n",
    "# Bayt website data scrapping\n",
    "**Note: All the links were collected on 20-12-2022**\n",
    "\n",
    "In this notebook we will scrape all the jobs data in bayt website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VasQn1BdlkUy"
   },
   "outputs": [],
   "source": [
    "# import all the needed libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import threading\n",
    "import requests\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All my Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7Gb1-RHl2ks"
   },
   "outputs": [],
   "source": [
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3', \n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36', \n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:54.0) Gecko/20100101 Firefox/54.0', \n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36', \n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36', \n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36', \n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; rv:78.0) Gecko/20100101 Firefox/78.0', \n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36', \n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.1 Safari/605.1.15', \n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 OPR/43.0.2442.991', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:65.0) Gecko/20100101 Firefox/65.0', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36', \n",
    "    'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0', \n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:15.0) Gecko/20100101 Firefox/15.0.1', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; rv:91.0) Gecko/20100101 Firefox/91.0', \n",
    "    'Mozilla/5.0 (Linux; Android 8.0.0; Pixel 2 XL Build/OPD1.170816.004) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Mobile Safari/537.36', \n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36', \n",
    "    'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1 Mobile/15E148 Safari/604.1', \n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.2 Safari/605.1.15', \n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36', \n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36', \n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36', \n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36', \n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.109 Safari/537.36', \n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/601.7.7 (KHTML, like Gecko) Version/9.1.2 Safari/601.7.7'\n",
    "]\n",
    "fieldnames= ['job_title', 'job_type', 'job_categories', 'job_description', 'job_location', 'company_name', 'company_location', 'company_field', 'company_size', 'experience_needed', 'career_level', 'education_level', 'gender', 'salary', 'skills_and_tools', 'link']\n",
    "errors_urls = [] # to collect all the urls that will fail\n",
    "companies = {} # add company data so if I collect the company data before I don't make another request to get the company info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjIyg6k3_yJE"
   },
   "outputs": [],
   "source": [
    "# read my links csv file\n",
    "df = pd.read_csv('/content/bayt_job_links_20-12-2022_1.csv', encoding='utf-16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and methods for getting and scrapping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBy2Tk-m3W3v"
   },
   "outputs": [],
   "source": [
    "# collect the company data by passing the company profile page on bayt\n",
    "def get_company_data(link):\n",
    "    # if I have collected the company data before get it and return it\n",
    "    if companies.get(link):\n",
    "        return companies.get(link)[0], companies.get(link)[1]\n",
    "\n",
    "    # request to get the company data\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(USER_AGENTS),\n",
    "        \"Accept-Encoding\": \"*\",\n",
    "        \"Connection\": \"keep-alive\"\n",
    "    }\n",
    "    response = requests.get(link, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # start collecting the data\n",
    "    company_size = None\n",
    "    company_location = None\n",
    "    company_block = soup.select_one('div.row.h-align-right.t-center-m')\n",
    "    if company_block:\n",
    "        for index, detail in enumerate(company_block.find_all('p', class_='d')):\n",
    "            if index == 0:\n",
    "            company_location = detail.text.strip()\n",
    "            elif index == 1:\n",
    "                company_size = detail.text.strip()\n",
    "        companies[link] = [company_size, company_location]\n",
    "\n",
    "    return company_size, company_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojgN6Mi6lylS"
   },
   "outputs": [],
   "source": [
    "# get all the data I want from the job details page\n",
    "def get_data(title, link, soup):\n",
    "\n",
    "    # check if job still active and if not return False\n",
    "    if soup.find(True, {'id': 'results_inner'}):\n",
    "        return False\n",
    "\n",
    "    \n",
    "    # Get the main page blocks\n",
    "    description = None\n",
    "    skills = None\n",
    "    job_details = None\n",
    "    preferred_candidate = None\n",
    "    main_blocks = soup.select('div.card-content.is-spaced')\n",
    "    for block in main_blocks:\n",
    "        if 'Description' in block.find('h2').text:\n",
    "            description = block\n",
    "        elif 'Skills' in block.find('h2').text:\n",
    "            skills = block\n",
    "        elif 'Details' in block.find('h2').text:\n",
    "            job_details = block\n",
    "        elif 'Candidate' in block.find('h2').text:\n",
    "            preferred_candidate = block\n",
    "\n",
    "            \n",
    "    # Get Job Description\n",
    "    job_description = None\n",
    "    if description:\n",
    "        job_description = description.get_text(separator=' ')\n",
    "\n",
    "\n",
    "    # Get Job Skills\n",
    "    skills_and_tools = None\n",
    "    if skills:\n",
    "        skills_and_tools = skills.get_text(separator=';')\n",
    "\n",
    "\n",
    "    # Get Job Details (job_locarion - campany_field - job_categories - job_types - salary)\n",
    "    job_location = None\n",
    "    company_field = None\n",
    "    job_categories = None\n",
    "    job_types = None\n",
    "    salary = None\n",
    "    if job_details:\n",
    "    details = job_details.find('dl').find_all('div')\n",
    "    for detail in details:\n",
    "        try:\n",
    "            if 'Job Location' in detail.dt.text:\n",
    "                job_location = detail.dd.text.strip()\n",
    "            elif 'Company Industry' in detail.dt.text:\n",
    "                company_field = detail.dd.text.strip()\n",
    "            elif 'Job Role' in detail.dt.text:\n",
    "                job_categories = detail.dd.text.strip()\n",
    "            elif 'Employment Type' in detail.dt.text:\n",
    "                job_types = detail.dd.text.strip()\n",
    "            elif 'Salary' in detail.dt.text:\n",
    "                salary = detail.dd.text.strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # Get Preferred Candidate (career_level - experience_needed - gender - education_level)\n",
    "    career_level = None\n",
    "    experience_needed = None\n",
    "    gender = 'both'\n",
    "    education_level = None\n",
    "    if preferred_candidate:\n",
    "        details = preferred_candidate.find('dl').find_all('div')\n",
    "        for detail in details:\n",
    "            if 'Career Level' in detail.dt.text:\n",
    "                career_level = detail.dd.text.strip()\n",
    "            elif 'Experience' in detail.dt.text:\n",
    "                experience_needed = detail.dd.text.strip()\n",
    "            elif 'Gender' in detail.dt.text:\n",
    "                gender = detail.dd.text.strip()\n",
    "            elif 'Degree' in detail.dt.text:\n",
    "                education_level = detail.dd.text.strip()\n",
    "\n",
    "\n",
    "    # Get Company Name\n",
    "    company_name = soup.find('h1', {'id': 'job_title'}).parent.select_one('li').text.strip()\n",
    "\n",
    "\n",
    "    # Get company location and company size \n",
    "    company_location = None\n",
    "    company_size = None\n",
    "    if company_name != 'Confidential Company':\n",
    "        company_link = f\"https://www.bayt.com{soup.find('h1', {'id': 'job_title'}).parent.find('a', class_='is-black').get('href')}\"\n",
    "        company_size, company_location = get_company_data(company_link)\n",
    "\n",
    "\n",
    "    data = {\n",
    "        'job_title': title,\n",
    "        'job_type': job_types,\n",
    "        'job_categories': job_categories,\n",
    "        'job_description': job_description,\n",
    "        'job_location': job_location,\n",
    "        'company_name': company_name,\n",
    "        'company_location': company_location,\n",
    "        'company_field': company_field,\n",
    "        'company_size': company_size,\n",
    "        'experience_needed': experience_needed,\n",
    "        'career_level': career_level,\n",
    "        'education_level': education_level,\n",
    "        'gender': gender,\n",
    "        'salary': salary,\n",
    "        'skills_and_tools': skills_and_tools,\n",
    "        'link': link\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51aAcDkI93O4"
   },
   "outputs": [],
   "source": [
    "# start collecting the data\n",
    "def start(mini_df):\n",
    "    # create a new csv file to append my data to\n",
    "    csv_file = open(f'bayt_job_data_20-12-2022.csv', 'a+', encoding=\"utf-16\")\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames, delimiter ='~')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # both the next variables used for preview puropose only to know the number of skipped urls and the progress of the loop\n",
    "    loop_index = 1\n",
    "    skipped_urls = 0\n",
    "    \n",
    "    # start looping throught all my links and get data\n",
    "    for index, row in mini_df.iterrows():\n",
    "        try:\n",
    "            print(f'Dataframe Index: {index} - Loop Index {loop_index}')\n",
    "            headers = {\n",
    "                'User-Agent': random.choice(USER_AGENTS),\n",
    "                \"Accept-Encoding\": \"*\",\n",
    "                \"Connection\": \"keep-alive\"\n",
    "            }\n",
    "            response = requests.get(row['link'].strip(), headers=headers)\n",
    "\n",
    "            soup =  BeautifulSoup(response.content, 'lxml')\n",
    "            \n",
    "            # collect my data using get_data function\n",
    "            page_data = get_data(title=row['title'], link=row['link'].strip(), soup=soup)\n",
    "\n",
    "            if page_data: # if page still exists and it returned a data add it to the csv file\n",
    "                writer.writerow(page_data)\n",
    "            loop_index += 1\n",
    "        except Exception as e:\n",
    "            skipped_urls += 1\n",
    "            print(f'Skip Number {skipped_urls} - Dataframe Index: {index} - Loop Index: {loop_index} - URL: {row[\"link\"]} \\n {e}')\n",
    "            errors_urls.append({\n",
    "                'link': row['link'],\n",
    "                'title': row['title']\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start my script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJYABmi3HXC5",
    "outputId": "7344753b-0efa-4f86-cacd-3017c21fb599"
   },
   "outputs": [],
   "source": [
    "start(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
