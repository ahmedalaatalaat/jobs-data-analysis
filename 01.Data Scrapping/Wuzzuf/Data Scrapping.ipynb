{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e20412",
   "metadata": {},
   "source": [
    "![wuzzuf logo](https://images.wuzzuf-data.net/files/training_programs/providers/wuzzuf-v1.png)\n",
    "\n",
    "# Wuzzuf website data scrapping\n",
    "**Note: All the links were collected on 20-12-2022**\n",
    "\n",
    "In this notebook we will scrape all the jobs data in wuzzuf website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Libraries\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import threading\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read my csv file to get all the collected links to get the data of this links\n",
    "df = pd.read_csv('wuzzuf_job_links_20-12-2022.csv', encoding='utf-16')\n",
    "errors_urls = [] # to collect all the urls that will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc899086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the data I want from the job details page\n",
    "def get_data(title, link, soup):\n",
    "    # Get Job Types\n",
    "    job_types = []\n",
    "    try:\n",
    "        for j_type in soup.find('div', class_='css-11rcwxl').find_all('a'):\n",
    "            job_types.append(j_type.text)\n",
    "        job_types = ';'.join(job_types)\n",
    "    except Exception:\n",
    "        job_types = None\n",
    "\n",
    "\n",
    "    # Get Job Categories\n",
    "    try:\n",
    "        job_categories = [] \n",
    "        for category in soup.find('ul', class_='css-h5dsne').find_all('li'):\n",
    "            job_categories.append(category.text)\n",
    "        job_categories = ';'.join(job_categories)\n",
    "    except Exception:\n",
    "        job_categories = None\n",
    "\n",
    "\n",
    "    # Get Job Description\n",
    "    try:\n",
    "        job_description = soup.find('div', class_='css-1uobp1k').text\n",
    "    except Exception:\n",
    "        job_description = None\n",
    "\n",
    "\n",
    "    # Get Job Requirements\n",
    "    try:\n",
    "        job_requirements = soup.find('div', class_='css-1t5f0fr').text\n",
    "    except Exception:\n",
    "        job_requirements = None\n",
    "\n",
    "\n",
    "    # Get Job Location\n",
    "    try:\n",
    "        job_location = soup.find('strong', class_='css-9geu3q').find(text=True, recursive=False)\n",
    "    except Exception:\n",
    "        job_location = None\n",
    "\n",
    "\n",
    "    # Get Company Name\n",
    "    try:\n",
    "        company_name = soup.find('strong', class_='css-9geu3q').find(text=True)\n",
    "    except Exception:\n",
    "        company_name = None\n",
    "\n",
    "\n",
    "    # Get Company Location\n",
    "    try:\n",
    "        company_location = soup.find('section', class_='css-1rhgoyg').find('span', class_='css-nhiaul').text.split('•')[0]\n",
    "    except Exception:\n",
    "        company_location = None\n",
    "\n",
    "\n",
    "    # Get Company Size\n",
    "    try:\n",
    "        company_size = soup.find('section', class_='css-1rhgoyg').find('span', class_='css-nhiaul').text.split('•')[1]\n",
    "    except Exception:\n",
    "        company_size = None\n",
    "\n",
    "\n",
    "    # Get Company Field\n",
    "    try:\n",
    "        company_field = soup.find('section', class_='css-1rhgoyg').find('span', class_='css-xilyze').text\n",
    "    except Exception:\n",
    "        company_field = None\n",
    "\n",
    "\n",
    "    # Get Job Details (Experience Needed - Career Level - Education Level - Gender - salary)\n",
    "    experience_needed = None\n",
    "    career_level = None\n",
    "    education_level = None\n",
    "    salary = None\n",
    "    gender = 'both'\n",
    "    try:\n",
    "        job_details_section = soup.find('section', class_='css-3kx5e2').find_all('div', class_='css-rcl8e5')\n",
    "        for detail in job_details_section:\n",
    "            try:\n",
    "                detail_title = detail.find('span', class_='css-wn0avc').text\n",
    "                if 'Experience' in detail_title:\n",
    "                    experience_needed = detail.find('span', class_='css-4xky9y').text\n",
    "                elif 'Career' in detail_title:\n",
    "                    career_level = detail.find('span', class_='css-4xky9y').text\n",
    "                elif 'Education' in detail_title:\n",
    "                    education_level = detail.find('span', class_='css-4xky9y').text\n",
    "                elif 'Gender' in detail_title:\n",
    "                    gender = detail.find('span', class_='css-4xky9y').text\n",
    "                elif 'Salary' in detail_title:\n",
    "                    salary = detail.find('span', class_='css-4xky9y').text\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Get Skills and Tools\n",
    "    try:\n",
    "        skills_and_tools = []\n",
    "        for skill in soup.find('div', class_='css-s2o0yh').find_all('span', class_='css-158icaa'):\n",
    "            skills_and_tools.append(skill.text)\n",
    "        skills_and_tools = ';'.join(skills_and_tools)\n",
    "    except Exception:\n",
    "        skills_and_tools = None\n",
    "\n",
    "    data = {\n",
    "        'job_title': title,\n",
    "        'job_type': job_types,\n",
    "        'job_categories': job_categories,\n",
    "        'job_description': job_description,\n",
    "        'job_requirements': job_requirements,\n",
    "        'job_location': job_location,\n",
    "        'company_name': company_name,\n",
    "        'company_location': company_location,\n",
    "        'company_field': company_field,\n",
    "        'company_size': company_size,\n",
    "        'experience_needed': experience_needed,\n",
    "        'career_level': career_level,\n",
    "        'education_level': education_level,\n",
    "        'gender': gender,\n",
    "        'salary': salary,\n",
    "        'skills_and_tools': skills_and_tools,\n",
    "        'link': link\n",
    "    }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1288e672",
   "metadata": {},
   "source": [
    "### The Thread Function\n",
    "* I am using Selenium because Wuzzuf needs to load the data after the page is loaded using JavaScript, so I need to wait until the data is loaded before I can scrape it. That's why I am using Selenium and the webdriver. The 5 seconds I gave for the driver to wait was determined after a series of trials to find the optimal time to wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53fbdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the task that will be ran in ascyn mode\n",
    "def start_driver_thread(mini_df, thread_number):\n",
    "    # start a chrom web driver \n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    # create a new csv per thread to add my data to\n",
    "    csv_file = open(f'data/wuzzuf_job_data_20-12-2022_{thread_number}.csv', 'a+', encoding=\"utf-16\")\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=['job_title', 'job_type', 'job_categories', 'job_description', 'job_requirements', 'job_location', 'company_name', 'company_location', 'company_field', 'company_size', 'experience_needed', 'career_level', 'education_level', 'gender', 'salary', 'skills_and_tools', 'link'], delimiter ='~')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # both the next variables used for preview puropose only to know the number of skipped urls and the progress of the loop\n",
    "    loop_index = 1\n",
    "    skipped_urls = 0\n",
    "    \n",
    "    # start collecting the data\n",
    "    for index, row in mini_df.iterrows():\n",
    "        # this loop is used to not allow the script from getting to the next page until it load the the javascript\n",
    "        while(True):\n",
    "            try:\n",
    "                print(f'Thread #{thread_number}: Dataframe Index: {index} - Loop Index {loop_index}')\n",
    "\n",
    "                # make a request to the page recive it and put it in soup object\n",
    "                driver.get(row['link'])\n",
    "                driver.implicitly_wait(5)\n",
    "                html = driver.page_source\n",
    "                soup =  BeautifulSoup(html, 'lxml')\n",
    "\n",
    "                # collect the page data using get_data function\n",
    "                page_data = get_data(title=row['title'], link=row['link'], soup=soup)\n",
    "\n",
    "                # some times it took more than 5 seconds for the page to load the data so if it not loaded these if condtion will catch it and will make the page load again with the same url\n",
    "                if page_data['job_categories'] == '' or page_data['job_categories'] == None:\n",
    "                    print(f'Thread #{thread_number}: Retring (no category) - Dataframe Index: {index} - Loop Index {loop_index} - URL: {row[\"link\"]}')\n",
    "                    continue\n",
    "                \n",
    "                # add data to the csv file\n",
    "                writer.writerow(page_data)\n",
    "                \n",
    "                loop_index += 1\n",
    "                break\n",
    "            except Exception as e:\n",
    "                skipped_urls += 1\n",
    "                print(f'Thread #{thread_number}: Skip Number {skipped_urls} - Dataframe Index: {index} - Loop Index: {loop_index} - URL: {row[\"link\"]} \\n {e}')\n",
    "                errors_urls.append({\n",
    "                    'link': row['link'],\n",
    "                    'title': row['title']\n",
    "                })\n",
    "                break\n",
    "    driver.close() # closeing the webdriver after finsishing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1288c8",
   "metadata": {},
   "source": [
    "### As you can see there are alot of pages that I need to scrape and it could even get bigger so for ease of use and time I have used mutithreads to split the data and make them run asyn \n",
    "#### How multithreading helped:\n",
    "According to the links CSV file, I have approximately 25k pages that I need to scrape. Because Wuzzuf needs to load data using JavaScript, I need the page to load. On average, if we say it takes 3 seconds for a page to load and another 5 seconds to load the data after the whole page is loaded (neglecting the time needed by Python to complete the task), it will take approximately 8 seconds per page. By doing simple math, **25000 * 8 = 200000 seconds = 3334 minutes = 56 hours = 2.3 days** to scrape the data.\n",
    "\n",
    "The solution was to use multithreading to divide the work into multiple tasks that run at the same time. In my case, I used 16 threads _(which may change depending on the ability of your PC and your Python version)_. By doing the same calculation but dividing it by 16 (the number of threads), it will only take approximately **208 minutes = 3.4 hours**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824cd059",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = np.array_split(df, 16) # split my main data frame to 16 data frames (number of threads that I will ran)\n",
    "\n",
    "# start runing the threads and gave each one the df that it will scrape\n",
    "threads = []\n",
    "for i, splited_df in enumerate(dfs):\n",
    "    thread = threading.Thread(target=start_driver_thread, kwargs={'mini_df': splited_df, 'thread_number': str(i + 1)})\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "    \n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
